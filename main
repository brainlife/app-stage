#!/usr/bin/env python

#PBS -l nodes=1:ppn=1
#PBS -l vmem=1gb
#PBS -l walltime=00:05:00
#PBS -N stage
##PBS -q normal
##PBS -A TG-DBS170009

import json
import subprocess
import errno
import os
import sys
import json

#TODO - I should validate paths specified in file src/dest to make sure it doesn't go outside the workdir

def makedirp(dir):
    try: 
	os.makedirs(dir)
    except OSError as exc:
	if exc.errno == errno.EEXIST and os.path.isdir(dir):
	    pass
	else:
	    raise

with open("config.json") as config_json:
    config = json.load(config_json)
    for dataset in config["datasets"]:

        print("staging %s" % dataset["id"])

        storage = "wrangler"
        if "storage" in dataset:
            storage = dataset["storage"]

        outdir=dataset["id"]
        if 'outdir' in dataset:
            outdir=dataset["outdir"]

        if storage == "wrangler" or storage == "osiris":
	    makedirp(outdir)
            src=os.environ["BRAINLIFE_ARCHIVE_"+storage]+"/"+dataset["project"]+"/"+dataset["id"]+".tar"
            code=subprocess.call(["tar", "xf", src, "-C", outdir])
            if code != 0:
                sys.exit(code)
	elif storage == "url":
	    makedirp(outdir)
            for file in dataset["storage_config"]["files"]:
                code=subprocess.call(["wget", "-O", outdir+"/"+file["local"], file["url"]])
                if code != 0:
                    sys.exit(code)

                #if .nii is found on the remote url, compress it to make it .nii.gz as 
                #all brainlife nifti file needs to be in .nii.gz (for openneuro)
                if file["url"].endswith(".nii"):
                    print("compressiong .nii to nii.gz")
                    tmpname=outdir+"/"+file["local"][:-3] #strip .gz
                    subprocess.call(["mv", outdir+"/"+file["local"], tmpname]) 
                    subprocess.call(["gzip", tmpname])

        elif storage == "datalad":
	    makedirp(outdir)
            for file in dataset["storage_config"]["files"]:
                src="/mnt/datalad/"+file["src"]
                print("datalad get", src)
                code=subprocess.call(["datalad", "get", "-d", "datasets.datalad.org", src])
                if code != 0:
                    sys.exit(code)

                #if .nii is found on the remote url, compress it to make it .nii.gz as 
                #all brainlife nifti file needs to be in .nii.gz (for openneuro)
                if src.endswith(".nii"):
                    print("compressiong .nii to nii.gz")
                    dest=outdir+"/"+file["dest"][:-3]
                    subprocess.call(["cp", src, dest]) 
                    subprocess.call(["gzip", "-f", dest]) 
                else:
                    subprocess.call(["ln", "-sf", src, outdir+"/"+file["dest"]]) 

        else:
            #download from brainlife download server
            #print(["bl", "dataset", "download", dataset["id"], outdir])
            code=subprocess.call(["bl", "dataset", "download", dataset["id"], outdir])
            if code != 0:
                sys.exit(code)
            
        #deprecated
        #check to see .brainlife.json exists (old dataset doesn't have it)
        #if not os.path.isfile(outdir+"/.brainlife.json"):
        #    print "no .brainlife.json.. creating substitute"
        #    #find the _outputs
        #    if "_outputs" in config:
        #        for output in config["_outputs"]:
        #            if "dataset_id" in output and "meta" in output and output["dataset_id"] == dataset["id"]:
        #                #print("using config._outputs")
        #                #print(output)
        #                with open(outdir+"/.brainlife.json", "w") as f:
        #                    json.dump(output, f)


